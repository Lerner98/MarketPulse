üìã WHAT'S INCLUDED:
Part 1: Trunk-Based Development

‚úÖ Why GitFlow is wrong for solo dev
‚úÖ Modern workflow (feat ‚Üí main)
‚úÖ Daily development flow

Part 2: Database Migrations

‚úÖ Alembic setup (step-by-step)
‚úÖ Async support (critical for FastAPI)
‚úÖ Auto-run on deployment
‚úÖ Rollback strategy

Part 3: Secrets Management

‚úÖ Doppler professional setup
‚úÖ Never commit secrets to Git
‚úÖ Rotate without redeploying
‚úÖ Audit trail

Part 4: Docker

‚úÖ One Dockerfile (dev + prod)
‚úÖ Multi-stage build
‚úÖ docker-compose for local dev
‚úÖ Environment parity

Part 5: Configuration

‚úÖ No hardcoded values
‚úÖ Everything from env vars
‚úÖ Single config.py file
‚úÖ Validation on startup

Part 6: Infrastructure as Code

‚úÖ render.yaml blueprint
‚úÖ Versioned in Git
‚úÖ One-click deploy
‚úÖ Pre-deploy migrations

Part 7: CI/CD

‚úÖ GitHub Actions workflow
‚úÖ Test on every PR
‚úÖ Auto-deploy to production
‚úÖ Deployment comments

Extras:

‚úÖ Complete troubleshooting guide
‚úÖ Interview Q&A preparation
‚úÖ Cost breakdown
‚úÖ What recruiters see
‚úÖ Success criteria checklist


üéØ USE THIS GUIDE:
Timeline:

After frontend integration completes (he's working on this now)
Before adding to resume (shows professional practices)
Deploy phase (~4 hours total)

Order:
1. Frontend Integration Guide (now)
2. Complete Production Guide (after frontend works)
3. Deploy to production (following guide)
4. Add to resume/LinkedIn
5. Send to recruiters

üí° KEY DIFFERENCES FROM WRONG GUIDE:
Wrong GuideCorrect Guidedevelop branchTrunk-based (feat ‚Üí main)Manual SQL scriptsAlembic auto-migrationsTwo DockerfilesOne DockerfileHardcoded configEnv vars only.env filesDoppler secretsManual dashboard clicksrender.yaml (IaC)Permanent stagingDocker local + Vercel previews

This guide is based on:

‚úÖ Real production practices at tech companies
‚úÖ 2025 best practices
‚úÖ Solo developer workflow
‚úÖ Free tier optimization
‚úÖ Interview-ready knowledge

Ready to use when frontend is complete. üöÄ




# üöÄ MarketPulse Production Deployment - The Complete Guide

**The Professional Way for Solo Developers (2025)**

**What This Guide Covers:**
- ‚úÖ Trunk-based development workflow
- ‚úÖ Alembic migrations (including async support)
- ‚úÖ Infrastructure as Code (render.yaml)
- ‚úÖ Proper secrets management
- ‚úÖ Single Dockerfile for dev + prod
- ‚úÖ CI/CD with GitHub Actions
- ‚úÖ Zero-cost deployment

**Time to Complete:** 3-4 hours  
**Skill Level:** Strong Junior ‚Üí Mid-level  
**Cost:** $0 (free tier everything)

---

## üéØ THE GOLDEN TRIO ARCHITECTURE

```
Frontend  ‚Üí Vercel (React/Vite CDN)
Backend   ‚Üí Render (Docker Container)
Database  ‚Üí Neon.tech (PostgreSQL)
```

**Why This Stack:**
- ‚úÖ Industry standard tools
- ‚úÖ True microservices architecture
- ‚úÖ $0 monthly cost
- ‚úÖ Professional DevOps practices
- ‚úÖ Never loses data (Neon doesn't delete)

---

## üîÑ PART 1: THE WORKFLOW (Trunk-Based Development)

### **What's Wrong with GitFlow**

**Old Way (WRONG for solo dev):**
```
main
  ‚Üì
develop ‚Üê You waste time syncing this
  ‚Üì
feature/xyz
```

**Problems:**
- ‚ùå Merge conflicts with yourself
- ‚ùå Staging environment burns free tier in 2 weeks
- ‚ùå Slow deployment cycle
- ‚ùå Unnecessary complexity

---

### **The Modern Way: Trunk-Based Development**

```
main (production) ‚Üê Only long-lived branch
  ‚Üì
  ‚îú‚îÄ feat/dashboard-quintiles
  ‚îú‚îÄ fix/hebrew-encoding
  ‚îî‚îÄ hotfix/cors-error
```

**Rules:**
1. ‚úÖ `main` is always deployable
2. ‚úÖ Feature branches live 1-3 days max
3. ‚úÖ Merge to `main` = automatic production deploy
4. ‚úÖ If you break production, fix forward (new PR)

**Why This is Professional:**
- Used by Google, Facebook, Amazon
- Faster iterations
- Forces you to keep `main` stable
- Encourages small, frequent commits

---

### **Daily Workflow:**

```bash
# Start new feature
git checkout main
git pull origin main
git checkout -b feat/add-phone-number

# Work, commit, work, commit
git add .
git commit -m "feat(users): add phone number field with validation"

# Push and create PR
git push origin feat/add-phone-number
# Open PR on GitHub against main

# After CI passes and review complete
# Merge via GitHub (squash and merge)

# Delete feature branch
git branch -d feat/add-phone-number
```

---

## üóÑÔ∏è PART 2: DATABASE MIGRATIONS (Alembic + Async)

### **Why You NEED Alembic**

**Without Alembic (Manual SQL):**
```sql
-- You add this manually
ALTER TABLE users ADD COLUMN phone_number VARCHAR(20);

-- Problems:
-- ‚ùå Not versioned in Git
-- ‚ùå Easy to forget to run
-- ‚ùå No rollback mechanism
-- ‚ùå Different between dev/prod
```

**With Alembic:**
```bash
# Generate migration automatically
alembic revision --autogenerate -m "add phone number"

# Apply migration
alembic upgrade head

# Rollback if needed
alembic downgrade -1
```

**Benefits:**
- ‚úÖ Migrations versioned in Git
- ‚úÖ Auto-generated from model changes
- ‚úÖ Rollback support
- ‚úÖ Consistent across environments

---

### **Setup Alembic (Step-by-Step)**

**Step 1: Install**

```bash
cd backend
pip install alembic asyncpg
pip freeze > requirements.txt
```

**Step 2: Initialize**

```bash
alembic init alembic
```

This creates:
```
backend/
‚îú‚îÄ‚îÄ alembic/
‚îÇ   ‚îú‚îÄ‚îÄ versions/        # Migration files go here
‚îÇ   ‚îú‚îÄ‚îÄ env.py          # We'll modify this
‚îÇ   ‚îî‚îÄ‚îÄ script.py.mako
‚îî‚îÄ‚îÄ alembic.ini         # Config file
```

---

### **Step 3: Configure for Async (CRITICAL)**

**Problem:** FastAPI uses `asyncpg` (async database). Default Alembic is sync.

**Solution:** Update `alembic/env.py` for async support.

**File: `backend/alembic/env.py`**

```python
"""Alembic environment configuration with async support"""
import asyncio
from logging.config import fileConfig

from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context

# Import your models so Alembic can detect changes
from api.models import Base  # Adjust import path to your models
from config import Config

# Alembic Config object
config = context.config

# Setup logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set metadata for autogenerate
target_metadata = Base.metadata

# Override sqlalchemy.url from environment
config.set_main_option("sqlalchemy.url", Config.DATABASE_URL)


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode (generates SQL file)"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    """Helper to run migrations with provided connection"""
    context.configure(
        connection=connection, 
        target_metadata=target_metadata
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_migrations_online() -> None:
    """Run migrations in 'online' mode (async support)"""
    # Create async engine
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    # Run migrations
    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


# Choose mode
if context.is_offline_mode():
    run_migrations_offline()
else:
    asyncio.run(run_migrations_online())
```

**What This Does:**
- ‚úÖ Supports async PostgreSQL (asyncpg)
- ‚úÖ Reads DATABASE_URL from environment
- ‚úÖ Auto-detects model changes
- ‚úÖ Works in production

---

### **Step 4: Create Your First Migration**

```bash
# Generate migration from model changes
alembic revision --autogenerate -m "initial schema"

# This creates: alembic/versions/abc123_initial_schema.py
```

**Review the generated file:**
```python
"""initial schema

Revision ID: abc123
Revises: 
Create Date: 2024-11-21 10:30:00
"""
from alembic import op
import sqlalchemy as sa

def upgrade():
    # Create tables
    op.create_table(
        'users',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('email', sa.String(255), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
    )

def downgrade():
    # Rollback
    op.drop_table('users')
```

**Apply migration:**
```bash
alembic upgrade head
```

---

### **Step 5: Automate Migrations on Deploy**

**File: `backend/docker-entrypoint.sh`**

```bash
#!/bin/bash
set -e

echo "üîÑ Running database migrations..."
python -m alembic upgrade head

if [ $? -eq 0 ]; then
    echo "‚úÖ Migrations completed successfully"
else
    echo "‚ùå Migration failed!"
    exit 1
fi

echo "üöÄ Starting application..."
exec "$@"
```

**Make executable:**
```bash
chmod +x backend/docker-entrypoint.sh
```

**Update Dockerfile to use it:**
```dockerfile
# ... (earlier Dockerfile content)

# Copy entrypoint
COPY --chown=appuser:appuser docker-entrypoint.sh /app/
RUN chmod +x /app/docker-entrypoint.sh

ENTRYPOINT ["/app/docker-entrypoint.sh"]
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Now every deploy:**
1. Runs migrations automatically
2. If migration fails ‚Üí deploy fails (safe!)
3. If migration succeeds ‚Üí app starts

---

## üîê PART 3: SECRETS MANAGEMENT (Doppler)

### **Why Doppler?**

**Without Doppler (BAD):**
```bash
# .env file (can't rotate, no audit trail)
DATABASE_URL=postgres://...
SECRET_KEY=abc123

# Or worse: Hardcoded in code
CORS_ORIGINS = ["https://marketpulse.vercel.app"]
```

**With Doppler (PROFESSIONAL):**
- ‚úÖ Centralized secrets management
- ‚úÖ Rotate without redeploying
- ‚úÖ Audit trail (who changed what, when)
- ‚úÖ Sync to Render/Vercel automatically
- ‚úÖ Team access control
- ‚úÖ Free tier (generous)

---

### **Setup Doppler (10 minutes)**

**Step 1: Create Account**

1. Go to [doppler.com](https://doppler.com)
2. Sign up (free, no credit card)
3. Create project: `marketpulse`

**Step 2: Add Secrets**

Create two environments: `dev` and `prd` (production)

**Development secrets:**
```
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/marketpulse_dev
ENVIRONMENT=development
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
SECRET_KEY=dev-secret-change-in-prod
```

**Production secrets:**
```
DATABASE_URL=postgres://user:pass@ep-xyz.neon.tech/neondb?sslmode=require
ENVIRONMENT=production
CORS_ORIGINS=https://marketpulse.vercel.app,https://marketpulse-git-*.vercel.app
SECRET_KEY=<generate: openssl rand -hex 32>
```

**Step 3: Install CLI**

```bash
# Mac
brew install dopplerhq/cli/doppler

# Linux
curl -sLf --retry 3 --tlsv1.2 --proto "=https" \
  'https://packages.doppler.com/public/cli/install.sh' | sh

# Windows
scoop install doppler
```

**Step 4: Login & Setup**

```bash
doppler login
cd /path/to/MarketPulse/backend
doppler setup
# Select: marketpulse ‚Üí dev
```

**Step 5: Run Locally**

```bash
# Instead of: python -m uvicorn api.main:app
doppler run -- python -m uvicorn api.main:app --reload

# Secrets automatically injected!
```

---

### **Step 6: Deploy with Doppler**

**Render:**

1. Get service token:
   ```bash
   doppler configs tokens create render-prod --config prd
   # Copy token: dp.st.prd.xyz123abc...
   ```

2. In Render dashboard ‚Üí Environment Variables:
   ```
   DOPPLER_TOKEN=dp.st.prd.xyz123abc...
   ```

3. Update Dockerfile:
   ```dockerfile
   # Install Doppler in image
   RUN apt-get update && apt-get install -y curl && \
       curl -sLf --retry 3 --tlsv1.2 --proto "=https" \
       'https://packages.doppler.com/public/cli/install.sh' | sh
   
   # Update entrypoint
   ENTRYPOINT ["doppler", "run", "--", "/app/docker-entrypoint.sh"]
   ```

**Vercel:**

1. Get service token for frontend:
   ```bash
   doppler configs tokens create vercel-prod --config prd
   ```

2. In Vercel dashboard ‚Üí Environment Variables:
   ```
   DOPPLER_TOKEN=dp.st.prd.xyz123abc...
   ```

3. In your build settings:
   ```json
   {
     "buildCommand": "doppler run -- npm run build"
   }
   ```

---

## üê≥ PART 4: THE PERFECT DOCKERFILE

### **One Dockerfile for Dev + Prod**

**File: `backend/Dockerfile`**

```dockerfile
# MarketPulse Backend - Production Dockerfile
# Multi-stage build optimized for size and security

FROM python:3.10-slim

WORKDIR /app

# Environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libpq-dev \
    curl \
    && curl -sLf --retry 3 --tlsv1.2 --proto "=https" \
       'https://packages.doppler.com/public/cli/install.sh' | sh \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user (security best practice)
RUN useradd -m appuser && \
    chown -R appuser:appuser /app && \
    chmod +x /app/docker-entrypoint.sh

USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Entrypoint runs migrations, then starts app
ENTRYPOINT ["doppler", "run", "--", "/app/docker-entrypoint.sh"]

# Default command (override for dev with --reload)
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**File: `backend/.dockerignore`**

```
__pycache__
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
.venv
.env
.env.*
.git
.gitignore
.pytest_cache
*.log
*.sqlite
.vscode/
.idea/
*.md
!README.md
tests/
docs/
data/raw/
alembic/versions/*.pyc
```

---

### **Local Development with Docker Compose**

**File: `docker-compose.yml`**

```yaml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: uvicorn api.main:app --host 0.0.0.0 --reload  # Hot reload for dev
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app  # Mount for hot reload
      - /app/__pycache__  # Don't sync cache
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/marketpulse
      - ENVIRONMENT=development
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173
    depends_on:
      db:
        condition: service_healthy
  
  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=marketpulse
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
  
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    command: npm run dev -- --host
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000

volumes:
  postgres_data:
```

**Run locally:**
```bash
docker-compose up
```

**Benefits:**
- ‚úÖ Identical to production (same Dockerfile)
- ‚úÖ Hot reload for development
- ‚úÖ PostgreSQL included
- ‚úÖ One command to start everything

---

## üìù PART 5: CONFIGURATION (No Hardcoding)

### **Single Config File with Environment Variables**

**File: `backend/config.py`**

```python
"""Application configuration from environment variables only"""
import os
from typing import List

class Config:
    """Environment-aware configuration"""
    
    # Environment
    ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
    DEBUG = ENVIRONMENT == "development"
    
    # Database
    DATABASE_URL = os.getenv("DATABASE_URL")
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL environment variable is required")
    
    # Replace postgresql:// with postgresql+asyncpg:// for async support
    if DATABASE_URL.startswith("postgresql://"):
        DATABASE_URL = DATABASE_URL.replace(
            "postgresql://", "postgresql+asyncpg://", 1
        )
    
    # API
    API_PREFIX = "/api"
    API_VERSION = "v1"
    API_TITLE = "MarketPulse API"
    
    # CORS - From comma-separated environment variable
    CORS_ORIGINS: List[str] = os.getenv(
        "CORS_ORIGINS",
        "http://localhost:3000,http://localhost:5173"
    ).split(",")
    
    # Strip whitespace from origins
    CORS_ORIGINS = [origin.strip() for origin in CORS_ORIGINS]
    
    # Security
    SECRET_KEY = os.getenv("SECRET_KEY")
    if ENVIRONMENT == "production" and not SECRET_KEY:
        raise ValueError("SECRET_KEY is required in production")
    
    # Logging
    LOG_LEVEL = "DEBUG" if DEBUG else "INFO"
    
    # Database pool settings
    DB_POOL_SIZE = 20 if ENVIRONMENT == "production" else 5
    DB_MAX_OVERFLOW = 40 if ENVIRONMENT == "production" else 10
    
    @classmethod
    def validate(cls):
        """Validate required configuration on startup"""
        required = ["DATABASE_URL"]
        
        if cls.ENVIRONMENT == "production":
            required.extend(["SECRET_KEY"])
        
        missing = []
        for key in required:
            if not getattr(cls, key, None):
                missing.append(key)
        
        if missing:
            raise ValueError(
                f"Missing required environment variables: {', '.join(missing)}"
            )
        
        # Log configuration (sanitized)
        print(f"üîß Environment: {cls.ENVIRONMENT}")
        print(f"üîß CORS Origins: {cls.CORS_ORIGINS}")
        print(f"üîß Database: {cls.DATABASE_URL.split('@')[1] if '@' in cls.DATABASE_URL else 'configured'}")

# Validate on import
Config.validate()
```

---

### **Use in FastAPI**

**File: `backend/api/main.py`**

```python
"""FastAPI application with environment-aware configuration"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from config import Config

# Create app
app = FastAPI(
    title=Config.API_TITLE,
    version=Config.API_VERSION,
    debug=Config.DEBUG,
)

# CORS from environment variables (NOT hardcoded)
app.add_middleware(
    CORSMiddleware,
    allow_origins=Config.CORS_ORIGINS,  # From env var
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """Health check endpoint for load balancers"""
    return {
        "status": "healthy",
        "environment": Config.ENVIRONMENT,
        "version": Config.API_VERSION
    }

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "MarketPulse API",
        "docs": "/docs",
        "health": "/health"
    }

# Import routers
from api.routers import cbs
app.include_router(cbs.router, prefix=Config.API_PREFIX)
```

---

## üèóÔ∏è PART 6: INFRASTRUCTURE AS CODE (render.yaml)

### **Why Infrastructure as Code?**

**Manual Dashboard (BAD):**
- ‚ùå Click buttons in Render dashboard
- ‚ùå Not versioned
- ‚ùå Can't reproduce
- ‚ùå Team members don't know config

**Infrastructure as Code (GOOD):**
- ‚úÖ Config in Git
- ‚úÖ Reproducible
- ‚úÖ Documented
- ‚úÖ Team can see all settings

---

### **File: `render.yaml`** (Root of repo)

```yaml
# MarketPulse Infrastructure Blueprint
# This file defines all infrastructure as code
# Deploy via: Render Dashboard ‚Üí Blueprints ‚Üí New Blueprint Instance

services:
  # Backend API Service
  - type: web
    name: marketpulse-backend
    runtime: docker
    repo: https://github.com/YOUR_USERNAME/MarketPulse  # ‚Üê Update this
    plan: free
    region: frankfurt  # or oregon (closest to users)
    branch: main
    dockerfilePath: ./backend/Dockerfile
    dockerContext: ./backend
    
    # Health check endpoint
    healthCheckPath: /health
    
    # Auto-deploy on push to main
    autoDeploy: true
    
    # Pre-deploy command (runs migrations before starting app)
    # This ensures database schema matches code
    preDeployCommand: "python -m alembic upgrade head"
    
    # Environment variables
    envVars:
      - key: PYTHON_VERSION
        value: "3.10.0"
      
      - key: ENVIRONMENT
        value: production
      
      # Secrets (set these in Render Dashboard for security)
      - key: DATABASE_URL
        sync: false  # Must be set manually (contains password)
      
      - key: SECRET_KEY
        sync: false  # Must be set manually
      
      - key: DOPPLER_TOKEN
        sync: false  # If using Doppler
      
      # Public configuration
      - key: CORS_ORIGINS
        value: "https://marketpulse.vercel.app,https://marketpulse-git-*.vercel.app"

# Note: Database is external (Neon.tech) for persistence
# Render's free DB deletes after 90 days - use Neon instead
```

---

### **How to Deploy with render.yaml**

**Step 1: Update the file**
```yaml
repo: https://github.com/GuyCohen85/MarketPulse  # Your actual repo
```

**Step 2: Commit to Git**
```bash
git add render.yaml
git commit -m "feat(infra): add render.yaml for infrastructure as code"
git push origin main
```

**Step 3: Deploy via Blueprint**

1. Go to [Render Dashboard](https://dashboard.render.com)
2. Click **Blueprints** in sidebar
3. Click **New Blueprint Instance**
4. Select your GitHub repository
5. Render auto-detects `render.yaml`
6. Click **Apply**

**Step 4: Add Secrets in Dashboard**

Render will create the service but needs you to add secrets:

1. Go to service ‚Üí **Environment**
2. Add:
   ```
   DATABASE_URL=postgres://user:pass@ep-xyz.neon.tech/neondb?sslmode=require
   SECRET_KEY=<openssl rand -hex 32>
   DOPPLER_TOKEN=dp.st.prd.xyz... (if using Doppler)
   ```

**Benefits:**
- ‚úÖ One-click redeploy (delete + recreate from yaml)
- ‚úÖ Team members can see all settings
- ‚úÖ Documented in Git
- ‚úÖ Professional DevOps practice

---

## ü§ñ PART 7: CI/CD WITH GITHUB ACTIONS

### **The Complete Workflow**

**File: `.github/workflows/deploy.yml`**

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  # Test backend
  backend-test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov
      
      - name: Run migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/test_db
        run: |
          cd backend
          alembic upgrade head
      
      - name: Run tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/test_db
          ENVIRONMENT: testing
          SECRET_KEY: test-secret-key
          CORS_ORIGINS: http://localhost:3000
        run: |
          cd backend
          pytest tests/ -v --cov=api --cov-report=term-missing
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./backend/coverage.xml
          flags: backend
  
  # Test frontend
  frontend-test:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Node
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Install dependencies
        run: |
          cd frontend
          npm ci
      
      - name: Run linter
        run: |
          cd frontend
          npm run lint
      
      - name: Run type check
        run: |
          cd frontend
          npm run type-check || true  # Don't fail build yet
      
      - name: Run tests
        run: |
          cd frontend
          npm test -- --passWithNoTests
      
      - name: Build check
        run: |
          cd frontend
          npm run build
  
  # Deploy to production (only on push to main after tests pass)
  deploy:
    needs: [backend-test, frontend-test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Trigger Render Deploy
        run: |
          curl -X POST "${{ secrets.RENDER_DEPLOY_HOOK }}"
      
      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v20
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          vercel-args: '--prod'
          working-directory: ./frontend
      
      - name: Comment deployment URLs
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: `üöÄ **Deployed to Production**
              
              - Frontend: https://marketpulse.vercel.app
              - Backend: https://marketpulse-api.onrender.com
              - API Docs: https://marketpulse-api.onrender.com/docs
              - Health: https://marketpulse-api.onrender.com/health`
            })
```

---

### **Setup GitHub Secrets**

**Required secrets in GitHub:**

1. Go to repo ‚Üí **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**
2. Add these secrets:

```
RENDER_DEPLOY_HOOK
  Value: https://api.render.com/deploy/srv-xyz?key=abc123
  (Get from: Render dashboard ‚Üí Service ‚Üí Settings ‚Üí Deploy Hook)

VERCEL_TOKEN
  Value: Get from https://vercel.com/account/tokens

VERCEL_ORG_ID
  Value: Get from Vercel project settings

VERCEL_PROJECT_ID
  Value: Get from Vercel project settings
```

---

## ‚úÖ COMPLETE DEPLOYMENT CHECKLIST

### **Phase 1: Project Setup (30 min)**
```
‚ñ° Create Neon database
‚ñ° Save DATABASE_URL
‚ñ° Setup Doppler (optional but recommended)
‚ñ° Add secrets to Doppler/GitHub
```

### **Phase 2: Alembic Setup (45 min)**
```
‚ñ° Install alembic: pip install alembic asyncpg
‚ñ° Run: alembic init alembic
‚ñ° Update alembic/env.py for async support
‚ñ° Configure config.py to use env vars
‚ñ° Create initial migration: alembic revision --autogenerate
‚ñ° Test locally: alembic upgrade head
‚ñ° Add docker-entrypoint.sh
‚ñ° Update Dockerfile with entrypoint
```

### **Phase 3: Docker Configuration (30 min)**
```
‚ñ° Create Dockerfile with multi-stage build
‚ñ° Create .dockerignore
‚ñ° Create docker-compose.yml for local dev
‚ñ° Install Doppler in Docker (if using)
‚ñ° Test locally: docker-compose up
‚ñ° Verify migrations run automatically
‚ñ° Verify app starts successfully
```

### **Phase 4: Infrastructure as Code (20 min)**
```
‚ñ° Create render.yaml in repo root
‚ñ° Update repo URL in render.yaml
‚ñ° Commit to Git
‚ñ° Deploy via Render Blueprints
‚ñ° Add DATABASE_URL in Render dashboard
‚ñ° Add SECRET_KEY in Render dashboard
‚ñ° Add DOPPLER_TOKEN in Render dashboard (if using)
```

### **Phase 5: Frontend Deployment (15 min)**
```
‚ñ° Create frontend/.env.production
‚ñ° Add VITE_API_URL=https://your-backend.onrender.com
‚ñ° Deploy to Vercel
‚ñ° Add VITE_API_URL in Vercel environment vars
‚ñ° Update CORS_ORIGINS in backend to include Vercel URL
‚ñ° Test frontend can reach backend
```

### **Phase 6: CI/CD (30 min)**
```
‚ñ° Create .github/workflows/deploy.yml
‚ñ° Add GitHub secrets (RENDER_DEPLOY_HOOK, etc.)
‚ñ° Push to main to trigger workflow
‚ñ° Verify tests run
‚ñ° Verify deployment succeeds
‚ñ° Check deployment comment on commit
```

### **Phase 7: Final Validation (20 min)**
```
‚ñ° Frontend loads: https://marketpulse.vercel.app
‚ñ° Backend health check: /health returns 200
‚ñ° API docs work: /docs shows Swagger UI
‚ñ° Hebrew displays correctly (not mojibake)
‚ñ° Database has 10,000 transactions
‚ñ° Migrations run automatically on deploy
‚ñ° Can create new feature branch and deploy
```

---

## üêõ TROUBLESHOOTING GUIDE

### **Problem: Alembic migration fails**

**Symptoms:**
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 
could not connect to server
```

**Solutions:**

**1. Check DATABASE_URL format:**
```python
# Wrong (sync):
postgresql://user:pass@host/db

# Correct (async):
postgresql+asyncpg://user:pass@host/db
```

**2. Verify SSL mode:**
```python
# Neon requires SSL
postgresql+asyncpg://user:pass@host/db?sslmode=require
```

**3. Test connection:**
```bash
python -c "
from sqlalchemy.ext.asyncio import create_async_engine
import asyncio

async def test():
    engine = create_async_engine('YOUR_DATABASE_URL')
    async with engine.connect() as conn:
        print('‚úÖ Connection successful')

asyncio.run(test())
"
```

---

### **Problem: Docker entrypoint doesn't run**

**Symptoms:**
- Migrations not running
- App starts but DB out of sync

**Solutions:**

**1. Check script is executable:**
```bash
chmod +x backend/docker-entrypoint.sh
```

**2. Check line endings (Windows users):**
```bash
# Convert CRLF to LF
dos2unix backend/docker-entrypoint.sh

# Or in VSCode: Change "CRLF" to "LF" in status bar
```

**3. Verify Dockerfile copies it:**
```dockerfile
COPY --chown=appuser:appuser docker-entrypoint.sh /app/
RUN chmod +x /app/docker-entrypoint.sh
```

---

### **Problem: CORS errors in browser**

**Symptoms:**
```
Access to fetch at 'https://api.onrender.com' from origin 
'https://app.vercel.app' has been blocked by CORS policy
```

**Solutions:**

**1. Check CORS_ORIGINS includes your frontend:**
```python
# In Render environment variables:
CORS_ORIGINS=https://marketpulse.vercel.app,https://marketpulse-git-*.vercel.app

# The wildcard (*) allows preview deployments
```

**2. Restart backend after changing CORS:**
```bash
# CORS is loaded on startup, so restart is required
# In Render: Manual Deploy ‚Üí Deploy latest commit
```

**3. Verify config loads correctly:**
```python
# Add logging to config.py
print(f"CORS Origins: {Config.CORS_ORIGINS}")

# Check Render logs to see what's loaded
```

---

### **Problem: Doppler secrets not loading**

**Symptoms:**
```
ValueError: DATABASE_URL environment variable is required
```

**Solutions:**

**1. Verify DOPPLER_TOKEN is set:**
```bash
# In Render dashboard
echo $DOPPLER_TOKEN
# Should show: dp.st.prd.xyz...
```

**2. Check Doppler is installed in Docker:**
```dockerfile
RUN curl -sLf --retry 3 --tlsv1.2 --proto "=https" \
    'https://packages.doppler.com/public/cli/install.sh' | sh
```

**3. Test Doppler locally:**
```bash
doppler run -- env | grep DATABASE_URL
# Should show your database URL
```

---

### **Problem: Render build fails**

**Symptoms:**
```
ERROR: failed to solve: failed to compute cache key
```

**Solutions:**

**1. Check Dockerfile path in render.yaml:**
```yaml
dockerfilePath: ./backend/Dockerfile  # Must match actual path
dockerContext: ./backend              # Context matters!
```

**2. Verify requirements.txt exists:**
```bash
ls backend/requirements.txt
# Should exist and contain all dependencies
```

**3. Check for missing dependencies:**
```bash
# Regenerate requirements.txt
cd backend
pip freeze > requirements.txt
git add requirements.txt
git commit -m "fix: update requirements"
git push
```

---

## üìä WHAT RECRUITERS SEE

### **On Your GitHub:**

```
‚úÖ render.yaml (Infrastructure as Code)
‚úÖ Alembic migrations (Professional DB management)
‚úÖ GitHub Actions CI/CD
‚úÖ Docker with multi-stage build
‚úÖ Trunk-based development (modern)
‚úÖ Comprehensive tests
‚úÖ Professional README
‚úÖ Live demo link
‚úÖ Architecture diagram
```

### **In Interviews:**

**Q:** "How do you handle database migrations in production?"

**Your Answer:**
> "I use Alembic for automated migrations. When I change a model, I run `alembic revision --autogenerate` which generates a migration file. This gets committed to Git. On deployment, my Docker entrypoint runs `alembic upgrade head` before starting the app, ensuring the database schema always matches the code. If a migration fails, the deployment fails, preventing broken deploys."

**Why this is strong:** Shows you understand:
- Versioned migrations
- Automated deployment
- Fail-fast strategy
- Git as source of truth

---

**Q:** "How do you manage secrets?"

**Your Answer:**
> "I use Doppler for centralized secrets management. Locally, developers run `doppler run` which injects secrets. In production, I set a single DOPPLER_TOKEN environment variable. This allows me to rotate secrets without redeploying code, provides an audit trail of changes, and makes it easy to onboard new team members. I never commit secrets to Git."

**Why this is strong:** Shows you understand:
- Security best practices
- Separation of config from code
- Audit requirements
- Team scalability

---

**Q:** "What's your deployment strategy?"

**Your Answer:**
> "I use trunk-based development with continuous deployment. All features branch from main and merge back via pull request. GitHub Actions runs tests on every PR. When merged to main, it automatically deploys to production via Render and Vercel. I use infrastructure as code with render.yaml so all configuration is versioned in Git. If something breaks, I fix forward with a new PR rather than rolling back."

**Why this is strong:** Shows you understand:
- Modern development practices
- CI/CD automation
- Infrastructure as code
- Fix-forward mentality

---

## üí∞ TOTAL COST

### **Free Tier Breakdown:**

| Service | Free Tier | Limits | Paid Upgrade |
|---------|-----------|--------|--------------|
| **Neon.tech** | $0 | 512MB storage, 1 compute unit | $19/mo for 10GB |
| **Render** | $0 | 750 hours/month, sleeps after 15min | $7/mo always-on |
| **Vercel** | $0 | Unlimited bandwidth (hobby) | $20/mo pro |
| **GitHub** | $0 | Unlimited public repos | Free |
| **Doppler** | $0 | 5 users, unlimited secrets | $8/user/mo |
| **TOTAL** | **$0/month** | Perfect for portfolio | ~$54/mo production |

---

## üéØ FINAL WORKFLOW SUMMARY

### **The Complete Professional Workflow:**

```
1. ‚úÖ Trunk-based development (feat ‚Üí main)
2. ‚úÖ Alembic migrations (auto-run on deploy)
3. ‚úÖ One Dockerfile (dev/prod parity)
4. ‚úÖ Config from environment variables
5. ‚úÖ Doppler for secrets management
6. ‚úÖ Infrastructure as code (render.yaml)
7. ‚úÖ CI/CD with GitHub Actions
8. ‚úÖ Health endpoint (/health)
9. ‚úÖ No staging environment (use Docker + Vercel previews)
10. ‚úÖ Professional error handling
```

### **Daily Development Flow:**

```bash
# Morning
git checkout main && git pull
git checkout -b feat/new-feature

# Work
# ... code, test, commit ...
doppler run -- python -m uvicorn api.main:app --reload

# Afternoon
git push origin feat/new-feature
# Create PR on GitHub

# CI runs automatically
# After approval, merge to main
# Automatic deployment to production
# Done!
```

---

## üìö ADDITIONAL RESOURCES

### **Learn More:**

- [Alembic Documentation](https://alembic.sqlalchemy.org/)
- [FastAPI Best Practices](https://github.com/zhanymkanov/fastapi-best-practices)
- [Trunk-Based Development](https://trunkbaseddevelopment.com/)
- [Doppler Documentation](https://docs.doppler.com/)
- [Render Documentation](https://render.com/docs)
- [The Twelve-Factor App](https://12factor.net/)

### **Recommended Video:**

[FastAPI with Async SQLAlchemy, Alembic and Docker](https://www.youtube.com/watch?v=EXAMPLE)  
*Covers the async Alembic setup which is the trickiest part*

---

## üèÜ SUCCESS CRITERIA

**You're done when you can:**

```
‚úÖ Push to main ‚Üí automatic deployment
‚úÖ Change a model ‚Üí migration auto-generated
‚úÖ Deploy ‚Üí migrations run automatically
‚úÖ Rotate secret in Doppler ‚Üí no redeploy needed
‚úÖ Create feature branch ‚Üí develop ‚Üí merge (< 1 day)
‚úÖ Show render.yaml to recruiter (infrastructure as code)
‚úÖ All tests pass in CI before deploy
‚úÖ Hebrew displays correctly in production
‚úÖ Zero-downtime deployments
‚úÖ Full audit trail of all changes
```

---

## üéâ ACHIEVEMENT UNLOCKED

**You've built a production-ready application with:**

- ‚úÖ Professional DevOps practices
- ‚úÖ Modern development workflow
- ‚úÖ Automated testing and deployment
- ‚úÖ Infrastructure as code
- ‚úÖ Proper secrets management
- ‚úÖ Database migration strategy
- ‚úÖ Zero-cost hosting

**This is EXACTLY what mid-level engineers do at tech companies.**

**You're no longer a junior. You're a strong mid-level developer.** üöÄ

---

**Time Investment:** 4-5 hours  
**Skill Gain:** Junior ‚Üí Mid-level  
**Portfolio Impact:** Massive  
**Interview Confidence:** Maximum

---

**End of Complete Production Guide**
